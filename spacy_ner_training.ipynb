{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(output_dir, nlp):\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n",
    "def load_model(output_dir):\n",
    "    print(\"Loading from\", output_dir)\n",
    "    nlp_model = spacy.load(output_dir)\n",
    "    return nlp_model\n",
    "\n",
    "def test_model(train_data, nlp):\n",
    "    for text, _ in train_data:\n",
    "        doc = nlp(text)\n",
    "        print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "        print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
    "\n",
    "def ner_trainer( train_data, config):\n",
    "\n",
    "    n_iter = config['n_iter']\n",
    "    model = config['model']\n",
    "\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank(\"en\")  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe(\"ner\")\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "    for _, annotations in train_data:\n",
    "        for ent in annotations.get(\"entities\"):\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "    # only train NER\n",
    "    with nlp.disable_pipes(*other_pipes), warnings.catch_warnings():\n",
    "        # show warnings for misaligned entity spans once\n",
    "        warnings.filterwarnings(\"once\", category=UserWarning, module='spacy')\n",
    "\n",
    "        if model is None:\n",
    "            nlp.begin_training()\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(train_data)\n",
    "            losses = {}\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            batches = minibatch(train_data, size=compounding(4.0, 32.0, 1.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(\n",
    "                    texts,  # batch of texts\n",
    "                    annotations,  # batch of annotations\n",
    "                    drop=0.5,  # dropout - make it harder to memorise data\n",
    "                    losses=losses,\n",
    "                )\n",
    "            print(\"Losses\", losses)\n",
    "\n",
    "    return nlp\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [\n",
    "    (\"Who is Shaka Khan?\", {\"entities\": [(7, 17, \"PERSON\")]}),\n",
    "    (\"I like London and Berlin.\", {\"entities\": [(7, 13, \"LOC\"), (18, 24, \"LOC\")]}),\n",
    "]\n",
    "\n",
    "config = {\n",
    "    'n_iter': 100,\n",
    "    'model': None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created blank 'en' model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manka\\Anaconda3\\lib\\site-packages\\spacy\\language.py:639: UserWarning: [W033] Training a new parser or NER using a model with no lexeme normalization table. This may degrade the performance of the model to some degree. If this is intentional or the language you're using doesn't have a normalization table, please ignore this warning. If this is surprising, make sure you have the spacy-lookups-data package installed. The languages with lexeme normalization tables are currently: da, de, el, en, id, lb, pt, ru, sr, ta, th.\n",
      "  **kwargs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 9.899998903274536}\n",
      "Losses {'ner': 9.691761136054993}\n",
      "Losses {'ner': 9.591479182243347}\n",
      "Losses {'ner': 9.285689115524292}\n",
      "Losses {'ner': 8.800917148590088}\n",
      "Losses {'ner': 8.058760046958923}\n",
      "Losses {'ner': 8.113943099975586}\n",
      "Losses {'ner': 7.526875972747803}\n",
      "Losses {'ner': 7.086795091629028}\n",
      "Losses {'ner': 7.3712087869644165}\n",
      "Losses {'ner': 6.647868990898132}\n",
      "Losses {'ner': 5.628359913825989}\n",
      "Losses {'ner': 5.779063642024994}\n",
      "Losses {'ner': 5.494428098201752}\n",
      "Losses {'ner': 5.153343170881271}\n",
      "Losses {'ner': 5.164166569709778}\n",
      "Losses {'ner': 4.061458259820938}\n",
      "Losses {'ner': 4.878674566745758}\n",
      "Losses {'ner': 4.588281527161598}\n",
      "Losses {'ner': 4.756910711526871}\n",
      "Losses {'ner': 4.621913880109787}\n",
      "Losses {'ner': 4.877700709737837}\n",
      "Losses {'ner': 3.6048336401581764}\n",
      "Losses {'ner': 4.3541263826191425}\n",
      "Losses {'ner': 4.475983753800392}\n",
      "Losses {'ner': 4.827014146372676}\n",
      "Losses {'ner': 3.93739452958107}\n",
      "Losses {'ner': 3.7484133914113045}\n",
      "Losses {'ner': 4.811004608869553}\n",
      "Losses {'ner': 3.376801174134016}\n",
      "Losses {'ner': 3.4543194323778152}\n",
      "Losses {'ner': 2.8000844083726406}\n",
      "Losses {'ner': 3.0439196191728115}\n",
      "Losses {'ner': 2.7901220098137856}\n",
      "Losses {'ner': 2.492804676294327}\n",
      "Losses {'ner': 2.9565262272953987}\n",
      "Losses {'ner': 2.1610535196959972}\n",
      "Losses {'ner': 2.034499667584896}\n",
      "Losses {'ner': 2.352672778069973}\n",
      "Losses {'ner': 1.6059322357177734}\n",
      "Losses {'ner': 1.9025870002806187}\n",
      "Losses {'ner': 4.548388309776783}\n",
      "Losses {'ner': 2.7201563492417336}\n",
      "Losses {'ner': 2.4341134633868933}\n",
      "Losses {'ner': 2.794638529419899}\n",
      "Losses {'ner': 2.46066802367568}\n",
      "Losses {'ner': 2.6224283762276173}\n",
      "Losses {'ner': 1.3467064090073109}\n",
      "Losses {'ner': 2.4149774499237537}\n",
      "Losses {'ner': 1.5181993332807906}\n",
      "Losses {'ner': 1.9472939998377115}\n",
      "Losses {'ner': 2.4587478103348985}\n",
      "Losses {'ner': 1.4529219260439277}\n",
      "Losses {'ner': 1.2263726443052292}\n",
      "Losses {'ner': 2.052179325837642}\n",
      "Losses {'ner': 2.5898521837807493}\n",
      "Losses {'ner': 1.4624417376180645}\n",
      "Losses {'ner': 0.8356330058304593}\n",
      "Losses {'ner': 1.2940083381254226}\n",
      "Losses {'ner': 1.0048521555872867}\n",
      "Losses {'ner': 1.2714140041498467}\n",
      "Losses {'ner': 1.2268056080210954}\n",
      "Losses {'ner': 0.964899849917856}\n",
      "Losses {'ner': 0.47523184752935776}\n",
      "Losses {'ner': 0.3565461227681226}\n",
      "Losses {'ner': 0.3507668053571251}\n",
      "Losses {'ner': 0.3140806505398359}\n",
      "Losses {'ner': 0.06544944345637305}\n",
      "Losses {'ner': 0.003132909138003015}\n",
      "Losses {'ner': 0.20667874024366029}\n",
      "Losses {'ner': 0.19349553568071443}\n",
      "Losses {'ner': 0.028930200544436957}\n",
      "Losses {'ner': 0.07635576634311292}\n",
      "Losses {'ner': 0.02433402007753216}\n",
      "Losses {'ner': 0.003443934963846651}\n",
      "Losses {'ner': 0.01305250848349715}\n",
      "Losses {'ner': 0.007125233245574236}\n",
      "Losses {'ner': 0.0037823006065735854}\n",
      "Losses {'ner': 0.0006766485988691784}\n",
      "Losses {'ner': 8.538141531744259e-05}\n",
      "Losses {'ner': 0.0004965890003783002}\n",
      "Losses {'ner': 0.0005356412940784594}\n",
      "Losses {'ner': 0.020388647047663272}\n",
      "Losses {'ner': 0.003282675210308561}\n",
      "Losses {'ner': 3.514747132271623e-05}\n",
      "Losses {'ner': 3.06991581873195e-05}\n",
      "Losses {'ner': 3.1789071179844086e-06}\n",
      "Losses {'ner': 3.10508167471954e-05}\n",
      "Losses {'ner': 5.242052436171428e-06}\n",
      "Losses {'ner': 0.0021944128773141447}\n",
      "Losses {'ner': 6.896612693488551e-05}\n",
      "Losses {'ner': 0.0005652407286229622}\n",
      "Losses {'ner': 3.9198506726734195e-05}\n",
      "Losses {'ner': 0.011943982382056156}\n",
      "Losses {'ner': 0.00020804129107455938}\n",
      "Losses {'ner': 0.00023845304945568913}\n",
      "Losses {'ner': 1.3336272379205205e-06}\n",
      "Losses {'ner': 6.128276870076554e-06}\n",
      "Losses {'ner': 0.00016208255453165066}\n",
      "Losses {'ner': 7.354283544464685e-05}\n",
      "Saved model to C:\\Users\\manka\\PycharmProjects\\ner\\model_data\n",
      "Loading from C:\\Users\\manka\\PycharmProjects\\ner\\model_data\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if not os.path.isdir('model_data'):\n",
    "    os.mkdir('model_data')\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "NEW_MODEL = os.path.join(current_dir, 'model_data')\n",
    "\n",
    "nlp = ner_trainer(train_data,config)\n",
    "save_model(NEW_MODEL, nlp)\n",
    "nlp_model = load_model(NEW_MODEL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities [('London', 'LOC'), ('Berlin', 'LOC')]\n",
      "Tokens [('I', '', 2), ('like', '', 2), ('London', 'LOC', 3), ('and', '', 2), ('Berlin', 'LOC', 3), ('.', '', 2)]\n",
      "Entities [('Shaka Khan', 'PERSON')]\n",
      "Tokens [('Who', '', 2), ('is', '', 2), ('Shaka', 'PERSON', 3), ('Khan', 'PERSON', 1), ('?', '', 2)]\n"
     ]
    }
   ],
   "source": [
    "test_model(train_data, nlp_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
